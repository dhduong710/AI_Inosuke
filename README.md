# AI-Inosuke Project

**Deployment link**: https://inosuke710-inosukeai710.hf.space/
Download **base model** here: https://huggingface.co/Qwen/Qwen2.5-3B-Instruct

## 1. Data Collection & Preprocessing

The dataset was handwritten and collected from anime, manga to replicate the **persona of Inosuke (Kimetsu no Yaiba)** in a natural way.  
All samples were normalized into the format:

```json
{"instruction": "...", "input": "...", "output": "..."}
```

### Dataset Statistics

| Source          | Samples |
|-----------------|---------|
| Persona         | 782     |
| Quotes          | 1,330   |
| Conversations   | 6,797   |
| Generic QA      | 350     |
| **Total**       | **9,259** |

**Dataset Distribution**

```mermaid
pie title Dataset Composition
  "Persona (8.4%)" : 782
  "Quotes (14.4%)" : 1330
  "Conversations (73.4%)" : 6797
  "Generic QA (3.8%)" : 350
```

- **Persona** ‚Üí Defines Inosuke‚Äôs characteristics, personality, and style.  
- **Quotes** ‚Üí Preserves original voice lines from the anime/manga.  
- **Conversations** ‚Üí Multi-turn dialogues, ensuring natural back-and-forth interactions.  
- **Generic QA** ‚Üí Covers common questions, boosting generalization.  

---

## 2. Qwen2.5 Instruct + QLoRA 4-bit

| Criterion | Explanation |
|-----------|-------------|
| **Base Model** | [Qwen2.5-3B Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct) ‚Äì lightweight yet powerful for dialogue tasks, with strong multilingual support including Vietnamese. |
| **Technique** | **QLoRA 4-bit** significantly reduces memory usage while maintaining performance close to full precision fine-tuning. |
| **Resources** | Optimized for **6GB VRAM GPUs (RTX 3060, etc.)**, making it feasible without high-end hardware. |
| **Efficiency** | Great balance between quality and compute efficiency, enabling persona training at scale. |

**QLoRA Workflow**

```mermaid
flowchart LR
    A[Base Model: Qwen2.5-3B Instruct] --> B[Quantization: 4-bit NF4]
    B --> C[LoRA Fine-tuning Layers]
    C --> D[Inosuke Persona Fine-tuned Model]
```

---

## 3. Fine-tuning Process

- **Framework**: Hugging Face Transformers + PEFT + BitsAndBytes  
- **Training Strategy**:  
  - LoRA adapters with 4-bit quantization  
  - Mixed precision (fp16) for efficiency  
  - Supervised fine-tuning (SFT)  
  - EarlyStopping (patience = 2)  
- **Training Setup**:
  - Epochs: 6  
  - Optimizer: `paged_adamw_32bit`  
  - Learning rate: 2e-4 (cosine scheduler, warmup ratio 0.05)  
  - Batch size per device: 1  
  - Gradient accumulation: 16 (effective batch size = 16)  
  - Weight decay: 0.01  
  - Max sequence length: 512 tokens  
  - Eval split: 10% (train/test split = 90/10)  

### Training Metrics

| Epoch | Training Loss | Eval Loss | Time per Epoch |
|-------|---------------|-----------|----------------|
| 1     | 4.04 ‚Üí 1.59   | 1.56      | ~02:01:20      |
| 2     | 1.59 ‚Üí 1.45   | 1.46      | ~02:09:33      |
| 3     | 1.45 ‚Üí 1.38   | 1.42      | ~02:09:01      |
| 4     | 1.38 ‚Üí 1.31   | 1.40      | ~02:09:05      |
| 5     | 1.31 ‚Üí 1.24   | 1.38      | ~02:09:19      |
| 6     | 1.24 ‚Üí 1.24   | 1.39      | ~02:17:52      |

The entire **fine-tuning process** took about **12 hours** in total.

The model started showing slight overfitting at epoch 6, so the **best checkpoint** was selected at **epoch 5**.

**Loss Curve Visualization**

![Loss Curve](loss_curve.png)

Both training and evaluation loss decrease steadily ‚Üí good convergence, no major signs of overfitting.   

---

## 4. Testing & Evaluation

To validate the fine-tuned Inosuke model, we performed both **automatic metrics** and **qualitative inspection**.

### Test Setup
- **Dataset**: Held-out test split (10%)  
- **Metrics**:
  - **BLEU** ‚Üí n-gram overlap (measures fluency/precision)  
  - **ROUGE-L** ‚Üí longest common subsequence (measures recall & similarity)  
  - **Embedding Similarity** ‚Üí cosine similarity using sentence embeddings  
  - **Perplexity (PPL)** ‚Üí model‚Äôs confidence in generating coherent sequences  

---

### üîπ Example Predictions

| Question | Ground Truth (GT) | Prediction (PR) | sim |
|----------|------------------|-----------------|-----|
| Xin ch√†o | Ch√†o! Ta l√† Inosuke, k·∫ª m·∫°nh nh·∫•t! | Hehe, ta l√† Inosuke! ƒê·ª´ng s·ª£ h√£i v·ªõi s·ª©c m·∫°nh c·ªßa ta! Ta s·∫Ω b·∫£o v·ªá ng∆∞∆°i h·∫øt c·∫£ ƒë·ªùi! Hahahaha! | 0.670 |
| Ng∆∞∆°i bi·∫øt Tanjiro kh√¥ng? | H·∫Øn m·∫°nh m·∫Ω nh∆∞ng qu√° hi·ªÅn l√†nh! Ta s·∫Ω ƒë·∫•u v·ªõi h·∫Øn m·ªôt ng√†y n√†o ƒë√≥! | Th·∫±ng ƒë√≥ hay la h√©t, nh∆∞ng ta th·∫•y n√≥ m·∫°nh m·∫Ω! Ta s·∫Ω h·ªçc t·ª´ n√≥. | 0.752 |
| N·∫øu c√≥ k·∫ª y·∫øu h∆°n ng∆∞∆°i th√¨ ng∆∞∆°i l√†m g√¨? | Ta s·∫Ω th·ª≠ ƒë·∫•u! N·∫øu th·∫Øng th√¨ vui, n·∫øu thua th√¨ ƒë√°nh l·∫°i cho ƒë·∫øn khi th·∫Øng! | Ta s·∫Ω h√©t v√†o m·∫∑t h·∫Øn, r·ªìi x√© to·∫°c h·∫Øn ra ƒë·ªÉ ch·ª©ng minh s·ª©c m·∫°nh th·∫≠t s·ª±! | 0.646 |
| Mi√™u t·∫£ c·∫£m gi√°c khi chi·∫øn th·∫Øng. | Gi·ªëng nh∆∞ l·ª≠a trong ng∆∞·ªùi b√πng ch√°y! V·∫øt th∆∞∆°ng kh√¥ng c√≤n quan tr·ªçng, ch·ªâ c√≤n ni·ªÅm vui h√©t th·∫≠t to! | Ng∆∞∆°i ƒë√£ ƒë√°nh b·∫°i ta, nh∆∞ng ta v·∫´n l√† k·∫ª m·∫°nh nh·∫•t! Ta s·∫Ω ch·ª©ng minh ƒëi·ªÅu ƒë√≥ cho ng∆∞∆°i th·∫•y. | 0.610 |
| Ng∆∞∆°i th√≠ch ƒÉn g√¨ nh·∫•t? | Th·ªãt n∆∞·ªõng! N∆∞·ªõng ch√°y c·∫°nh m·ªõi ngon! | N·∫øu ph·∫£i ch·ªçn, th√¨ c∆°m tr·∫Øng v√† th·ªãt n∆∞·ªõng! Ng·ª±c ta rung l√™n khi nghƒ© ƒë·∫øn m√πi th∆°m ·∫•y. | 0.592 |

---

### üîπ Quantitative Results

| Metric | Score |
|--------|-------|
| **BLEU** | 0.0000 |
| **ROUGE-L** | 0.3281 |
| **Embedding Similarity** | 0.6541 |
| **Perplexity (PPL)** | 8.23 |

---

### üîπ Analysis
- **BLEU is near zero** ‚Üí expected, since the model does not copy word-for-word but generates stylistic, aggressive responses.  
- **ROUGE-L (0.33)** ‚Üí moderate overlap with ground truth, showing consistency in content.  
- **Embedding Similarity (0.65)** ‚Üí good semantic alignment with intended answers.  
- **Perplexity (8.23)** ‚Üí indicates reasonable fluency and coherence for dialogue tasks.  

Overall, the model successfully captures **Inosuke‚Äôs personality** (loud, aggressive, primal) while maintaining coherence, even if literal overlap with ground truth is low.

---

## 5. Deployment

After fine-tuning, the **Inosuke Persona Model** can be deployed as a web application accessible either locally or on **Hugging Face Spaces (Free 16GB CPU)**.

### Technologies Used

- **Flask** ‚Üí backend server for chat API and UI rendering  
- **HTML + CSS** ‚Üí simple frontend for user interaction  
- **Transformers + PEFT** ‚Üí load the base model and apply LoRA adapters  
- **Docker** ‚Üí containerize the whole application for reproducible deployment  
- **Hugging Face Spaces** ‚Üí free hosting platform for public access  

### Local Deployment

1. Create and activate a virtual environment.  
2. Install dependencies from `requirements.txt`.  
3. Run the Flask application (`app.py`).  
4. Access the chatbot in the browser via `http://127.0.0.1:7860`.  

### Hugging Face Deployment

1. Create a new **Space** on Hugging Face.  
2. Select **Docker** as the runtime.  
3. Push the deploy folder with models/checkpoint.  
4. Hugging Face will automatically build and serve the app.  
5. The chatbot becomes accessible publicly with no extra setup.  

---


